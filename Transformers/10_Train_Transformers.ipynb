{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed06771f",
   "metadata": {},
   "source": [
    "# Training Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb093abf",
   "metadata": {},
   "source": [
    "Dataset - https://github.com/Rishav09/Neural-Machine-Translation-System/blob/master/english-german-both.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23699e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "tf.Tensor([0. 0. 0. 0. 1. 1. 1.], shape=(7,), dtype=float32)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 5, 512)]     0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_123 (Mult  (None, 5, 512)      131776      ['input_3[0][0]',                \n",
      " iHeadAttention)                                                  'input_3[0][0]',                \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_223 (Dropout)          (None, 5, 512)       0           ['multi_head_attention_123[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " add_normalization_209 (AddNorm  (None, 5, 512)      1024        ['input_3[0][0]',                \n",
      " alization)                                                       'dropout_223[0][0]']            \n",
      "                                                                                                  \n",
      " feed_forward_86 (FeedForward)  (None, 5, 512)       2099712     ['add_normalization_209[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_224 (Dropout)          (None, 5, 512)       0           ['feed_forward_86[0][0]']        \n",
      "                                                                                                  \n",
      " add_normalization_210 (AddNorm  (None, 5, 512)      1024        ['add_normalization_209[0][0]',  \n",
      " alization)                                                       'dropout_224[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,233,536\n",
      "Trainable params: 2,233,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 5, 512)]     0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_124 (Mult  (None, 5, 512)      131776      ['input_4[0][0]',                \n",
      " iHeadAttention)                                                  'input_4[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_225 (Dropout)          (None, 5, 512)       0           ['multi_head_attention_124[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " add_normalization_211 (AddNorm  (None, 5, 512)      1024        ['input_4[0][0]',                \n",
      " alization)                                                       'dropout_225[0][0]',            \n",
      "                                                                  'add_normalization_211[0][0]',  \n",
      "                                                                  'dropout_226[0][0]']            \n",
      "                                                                                                  \n",
      " multi_head_attention_125 (Mult  (None, 5, 512)      131776      ['add_normalization_211[0][0]',  \n",
      " iHeadAttention)                                                  'input_4[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_226 (Dropout)          (None, 5, 512)       0           ['multi_head_attention_125[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " feed_forward_87 (FeedForward)  (None, 5, 512)       2099712     ['add_normalization_211[1][0]']  \n",
      "                                                                                                  \n",
      " dropout_227 (Dropout)          (None, 5, 512)       0           ['feed_forward_87[0][0]']        \n",
      "                                                                                                  \n",
      " add_normalization_213 (AddNorm  (None, 5, 512)      1024        ['add_normalization_211[1][0]',  \n",
      " alization)                                                       'dropout_227[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,365,312\n",
      "Trainable params: 2,365,312\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy.random import shuffle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import convert_to_tensor, int64\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow import data, train, math, reduce_sum, cast, equal, argmax, \\\n",
    "float32, GradientTape, TensorSpec, function, int64\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow import math, cast, float32, linalg, ones, maximum, newaxis\n",
    "%run 09_Transformer_Encoder_Decoder.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccd58b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/a.daggula/Downloads/english-german-both.pkl'\n",
    "clean_dataset = load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd38aeb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['i like both', 'ich mag beide'],\n",
       "       ['she misses him', 'er fehlt ihr'],\n",
       "       ['i followed him', 'ich folgte ihm'],\n",
       "       ['its unusual', 'es ist ungewohnlich'],\n",
       "       ['she sounded mad', 'sie klang wutend']], dtype='<U370')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_dataset[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1edbec51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fab34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset:\n",
    "    def __init__(self,**kwargs):\n",
    "        self.n_sentences = 9999\n",
    "        self.train_split = 0.9\n",
    "    \n",
    "    def create_tokenizer(self, dataset):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "        return tokenizer\n",
    "    \n",
    "    def find_seq_length(self, dataset):\n",
    "        return max(len(seq.split()) for seq in dataset)\n",
    "    \n",
    "    def find_vocab_size(self, tokenizer, dataset):\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "        return len(tokenizer.word_index) + 1\n",
    "    \n",
    "    def __call__(self, filename, **kwargs):\n",
    "        # Load a Dataset\n",
    "        clean_dataset = load(open(filename, 'rb'))\n",
    "        \n",
    "        # Reduce Dataset Size\n",
    "        dataset = clean_dataset[:self.n_sentences, :]\n",
    "        \n",
    "        # Add Start and Stop Tokens\n",
    "        for i in range(dataset[:,0].size):\n",
    "            dataset[i, 0] = \"<START>\" + dataset[i, 0] + \"<EOS>\"\n",
    "            dataset[i, 1] = \"<START>\" + dataset[i, 1] + \"<EOS>\"\n",
    "        \n",
    "        # Random Shuffle the dataset\n",
    "        shuffle(dataset)\n",
    "        \n",
    "        # Split the dataset\n",
    "        train = dataset[:int(self.n_sentences * self.train_split)]\n",
    "        \n",
    "        # Prepare tokenizer for the encoder input\n",
    "        enc_tokenizer = self.create_tokenizer(train[:, 0])\n",
    "        enc_seq_length = self.find_seq_length(train[:, 0])\n",
    "        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n",
    "        \n",
    "        # Encode and pad the input sequences\n",
    "        trainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n",
    "        trainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\n",
    "        trainX = convert_to_tensor(trainX, dtype=int64)\n",
    "        \n",
    "        #Prepare tokenizer for the decoder input\n",
    "        dec_tokenizer = self.create_tokenizer(train[:, 1])\n",
    "        dec_seq_length = self.find_seq_length(train[:, 1])\n",
    "        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n",
    "        \n",
    "        # Encode and pad the input sequences\n",
    "        trainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n",
    "        trainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\n",
    "        trainY = convert_to_tensor(trainY, dtype=int64)\n",
    "        \n",
    "        return (trainX, trainY, train, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d40c5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START>they swam<EOS> \n",
      " tf.Tensor([  1  49 595   2   0], shape=(5,), dtype=int64)\n",
      "<START>ich liebe meine familie<EOS> \n",
      " tf.Tensor([  1   3  56  57 484   2   0   0   0   0], shape=(10,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = PrepareDataset()\n",
    "trainX, trainY, train_orig, enc_seq_length, dec_seq_length, \\\n",
    "     enc_vocab_size, dec_vocab_size = dataset(filename)\n",
    "\n",
    "print(train_orig[20, 0], '\\n', trainX[20, :])\n",
    "print(train_orig[0, 1], '\\n', trainY[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7c26b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder sequence length: 5\n",
      "Decoder sequence length: 10\n"
     ]
    }
   ],
   "source": [
    "print('Encoder sequence length:', enc_seq_length)\n",
    "print('Decoder sequence length:', dec_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697bf26",
   "metadata": {},
   "source": [
    "Apply Padding Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c0d5ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fcn(target, prediction):\n",
    "    mask = math.logical_not(equal(target, 0))\n",
    "    mask = cast(mask, float32)\n",
    "    \n",
    "    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * mask\n",
    "    return reduce_sum(loss) / reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae20399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fcn(target, prediction):\n",
    "    mask = math.logical_not(math.equal(target,0))\n",
    "    \n",
    "    # Find equal prediction and target values, and apply the padding mask\n",
    "    accuracy = equal(target, argmax(prediction, axis=2))\n",
    "    accuracy = math.logical_and(mask, accuracy)\n",
    "    # Cast the True/False values to 32-bit-precision floating-point numbers\n",
    "    mask = cast(mask, float32)\n",
    "    accuracy = cast(accuracy, float32)\n",
    "    # Compute the mean accuracy over the unmasked values\n",
    "    return reduce_sum(accuracy) / reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a66a2",
   "metadata": {},
   "source": [
    "#### Train the transformer Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3dc17e3",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the model parameters\n",
    "h = 8 # Number of self-attention heads\n",
    "d_k = 64 # Dimensionality of the linearly projected queries and keys d_v = 64 # Dimensionality of the linearly projected values\n",
    "d_model = 512 # Dimensionality of model layers' outputs\n",
    "d_ff = 2048 # Dimensionality of the inner fully connected layer\n",
    "n = 6 # Number of layers in the encoder stack\n",
    "# Define the training parameters\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "dropout_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75ff6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler(LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = cast(d_model, float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step_num):\n",
    "        # Linearly increasing the learning rate for the first warmup_steps, and # decreasing it thereafter\n",
    "        arg1 = step_num ** -0.5\n",
    "        arg2 = step_num * (self.warmup_steps ** -1.5)\n",
    "        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06088071",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e6fcff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e748a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length,\n",
    "                                   dec_seq_length, h, d_k, d_v, d_model, d_ff, n,\n",
    "                                   dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4a639",
   "metadata": {},
   "source": [
    "#### TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb6a952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function\n",
    "def train_step(encoder_input, decoder_input, decoder_ouput):\n",
    "    with GradientTape() as tape:\n",
    "        # Run the forward pass of the model to generate a prediction\n",
    "        prediction = training_model(encoder_input, decoder_input, training=True)\n",
    "\n",
    "        # Compute a training loss\n",
    "        loss = loss_fcn(decoder_input, prediction)\n",
    "        \n",
    "        # Compute the training_accuracy\n",
    "        accuracy = accuracy_fcn(decoder_output, prediction)\n",
    "    \n",
    "    # Retrieve gradients of the trainable weights with respect to the training loss\n",
    "    gradients = tape.gradient(loss, training_model.trainable_weights)\n",
    "    \n",
    "    # Update the values of the trainable variables by gradient descent\n",
    "    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6875955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n",
      "Epoch 1 Step 0 Loss 8.1550 Accuracy 0.0000\n",
      "Epoch 1 Step 50 Loss 7.2856 Accuracy 0.0169\n",
      "Epoch 1 Step 100 Loss 6.5785 Accuracy 0.0847\n",
      "Epoch 1: Training Loss 6.1463, Training Accuracy 0.1192\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0 Loss 4.8509 Accuracy 0.2066\n",
      "Epoch 2 Step 50 Loss 4.3457 Accuracy 0.1792\n",
      "Epoch 2 Step 100 Loss 4.0256 Accuracy 0.1527\n",
      "Epoch 2: Training Loss 3.8054, Training Accuracy 0.1401\n"
     ]
    }
   ],
   "source": [
    "train_loss = Mean(name='train_loss')\n",
    "train_accuracy = Mean(name='train_accuracy')\n",
    "\n",
    "\n",
    "# Create a checkpoint object and manager to manage multiple checkpoints\n",
    "ckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\n",
    "ckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=3)\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    train_loss.reset_states() \n",
    "    train_accuracy.reset_states()\n",
    "    print(\"\\nStart of epoch %d\" % (epoch + 1))\n",
    "    \n",
    "    # Iterate over the dataset batches\n",
    "    for step, (train_batchX, train_batchY) in enumerate(train_dataset): \n",
    "        # Define the encoder and decoder inputs, and the decoder output \n",
    "        encoder_input = train_batchX[:, 1:]\n",
    "        decoder_input = train_batchY[:, :-1]\n",
    "        decoder_output = train_batchY[:, 1:]\n",
    "        train_step(encoder_input, decoder_input, decoder_output)\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} '\n",
    "            + f'Accuracy {train_accuracy.result():.4f}')\n",
    "    \n",
    "    # Print epoch number and loss value at the end of every epoch\n",
    "    print(f\"Epoch {epoch +1}: Training Loss {train_loss.result():.4f}, \" + f\"Training Accuracy {train_accuracy.result():.4f}\")\n",
    "    # Save a checkpoint after every five epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_path = ckpt_manager.save()\n",
    "        print(\"Saved checkpoint at epoch %d\" % (epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa06459c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2aad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
