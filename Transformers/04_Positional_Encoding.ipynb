{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c85ab18",
   "metadata": {},
   "source": [
    "# Custom Implementation of PositionEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63b4273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b1fa0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPositionEncoding(seq_len, d, n=10000):\n",
    "    P = np.zeros((seq_len,d))\n",
    "    for k in range(seq_len):\n",
    "        for i in np.arange(int(d/2)):\n",
    "            denominator = np.power(n, 2*i/d)\n",
    "            P[k,2*i] = np.sin(k/denominator)\n",
    "            P[k,2*i+1] = np.cos(k/denominator)\n",
    "            \n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32486edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          1.          0.          1.        ]\n",
      " [ 0.84147098  0.54030231  0.09983342  0.99500417]\n",
      " [ 0.90929743 -0.41614684  0.19866933  0.98006658]\n",
      " [ 0.14112001 -0.9899925   0.29552021  0.95533649]]\n"
     ]
    }
   ],
   "source": [
    "P = getPositionEncoding(seq_len=4, d=4, n=100)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f670b3b3",
   "metadata": {},
   "source": [
    "## Positional Encoding Layer in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78c340f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import convert_to_tensor, string\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, Layer\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12e781b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['', '[UNK]', 'robot', 'you', 'too', 'i', 'am', 'a']\n",
      "Vectorized words:  tf.Tensor(\n",
      "[[5 6 7 2 0]\n",
      " [3 4 2 0 0]], shape=(2, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "output_sequence_length = 5\n",
    "vocab_size = 10\n",
    "sentences = [['i am a robot'],['you too robot']]\n",
    "sentence_data = Dataset.from_tensor_slices(sentences)\n",
    "\n",
    "#Create Text Vectorization layer\n",
    "vectorize_layer = TextVectorization(output_sequence_length=output_sequence_length,\n",
    "                                   max_tokens=vocab_size)\n",
    "\n",
    "#Train the layer to create a dictionary\n",
    "vectorize_layer.adapt(sentence_data)\n",
    "\n",
    "#convert all senetences to tensors\n",
    "word_tensors = convert_to_tensor(sentences,dtype=tf.string)\n",
    "\n",
    "# Use the word tensors to get vectorized phrases \n",
    "vectorized_words = vectorize_layer(word_tensors)\n",
    "\n",
    "print(\"Vocabulary: \", vectorize_layer.get_vocabulary()) \n",
    "print(\"Vectorized words: \", vectorized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416de84b",
   "metadata": {},
   "source": [
    "#### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3fc841f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.01465261 -0.02443929 -0.02567568  0.01612092 -0.0180501\n",
      "    0.01221377]\n",
      "  [ 0.04108001 -0.0425668   0.03379712  0.02587621  0.04580636\n",
      "    0.01414463]\n",
      "  [ 0.03896799  0.03274581 -0.01577972  0.01959071  0.03781739\n",
      "   -0.0205436 ]\n",
      "  [ 0.04765031  0.04046229  0.03540636 -0.04586948 -0.04644794\n",
      "   -0.03318375]\n",
      "  [-0.037299   -0.03110769  0.02678451 -0.00517948  0.04170302\n",
      "   -0.02068119]]\n",
      "\n",
      " [[ 0.02332988  0.036721   -0.00412661 -0.00083592  0.0074178\n",
      "   -0.02291187]\n",
      "  [-0.01510976 -0.03449613  0.04011663  0.01848065 -0.00095644\n",
      "   -0.00769093]\n",
      "  [ 0.04765031  0.04046229  0.03540636 -0.04586948 -0.04644794\n",
      "   -0.03318375]\n",
      "  [-0.037299   -0.03110769  0.02678451 -0.00517948  0.04170302\n",
      "   -0.02068119]\n",
      "  [-0.037299   -0.03110769  0.02678451 -0.00517948  0.04170302\n",
      "   -0.02068119]]], shape=(2, 5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "output_length = 6\n",
    "word_embedding_layer = Embedding(vocab_size, output_length)\n",
    "embedded_words = word_embedding_layer(vectorized_words)\n",
    "print(embedded_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bf45e4",
   "metadata": {},
   "source": [
    "#### Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "686f136f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.02561091 -0.03799593 -0.01213596 -0.03664383  0.0425745  -0.03410308]\n",
      " [ 0.03241188  0.03786621 -0.04308493 -0.02314249  0.03154607 -0.0133027 ]\n",
      " [-0.02864889 -0.00165879  0.01828745  0.01672513  0.03201916  0.00918214]\n",
      " [-0.01671242  0.04078529 -0.02116981 -0.02729063 -0.03995778 -0.03068408]\n",
      " [-0.01710922 -0.0359491  -0.02147167  0.04926025 -0.03485603  0.02914684]], shape=(5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "position_embedding_layer = Embedding(output_sequence_length, output_length)\n",
    "position_indices = tf.range(output_sequence_length)\n",
    "embedded_indices = position_embedding_layer(position_indices)\n",
    "print(embedded_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c3d08",
   "metadata": {},
   "source": [
    "#### Output of Positional Encoding Layer in Transformers\n",
    "Pos.Encoding = Word_Embedding + Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5c12cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output:  tf.Tensor(\n",
      "[[[ 0.0109583  -0.06243522 -0.03781164 -0.0205229   0.0245244\n",
      "   -0.02188932]\n",
      "  [ 0.07349189 -0.00470059 -0.00928781  0.00273372  0.07735243\n",
      "    0.00084194]\n",
      "  [ 0.0103191   0.03108703  0.00250772  0.03631584  0.06983655\n",
      "   -0.01136146]\n",
      "  [ 0.0309379   0.08124758  0.01423656 -0.07316011 -0.08640572\n",
      "   -0.06386783]\n",
      "  [-0.05440821 -0.06705679  0.00531284  0.04408077  0.00684699\n",
      "    0.00846565]]\n",
      "\n",
      " [[ 0.04894079 -0.00127493 -0.01626257 -0.03747974  0.0499923\n",
      "   -0.05701496]\n",
      "  [ 0.01730212  0.00337008 -0.0029683  -0.00466185  0.03058963\n",
      "   -0.02099362]\n",
      "  [ 0.01900142  0.0388035   0.05369381 -0.02914435 -0.01442878\n",
      "   -0.02400161]\n",
      "  [-0.05401142  0.0096776   0.0056147  -0.03247011  0.00174524\n",
      "   -0.05136527]\n",
      "  [-0.05440821 -0.06705679  0.00531284  0.04408077  0.00684699\n",
      "    0.00846565]]], shape=(2, 5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "final_output_embedding = embedded_words + embedded_indices\n",
    "print(\"Final output: \", final_output_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bcd9fe",
   "metadata": {},
   "source": [
    "### Subclassing the Keras Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef7a0b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingLayer(Layer):\n",
    "    def __init__(self, seq_length, vocab_size, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.word_embedding_layer = Embedding(\n",
    "                                    input_dim=vocab_size,\n",
    "                                    output_dim=output_dim)\n",
    "        self.position_embedding_layer = Embedding(\n",
    "                                    input_dim=seq_length,\n",
    "                                    output_dim=output_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e220ab07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from my_embedded_layer: tf.Tensor(\n",
      "[[[ 0.00637855  0.00673728 -0.03841537  0.07975792 -0.04048951\n",
      "   -0.04499073]\n",
      "  [-0.00443989  0.01852877  0.02897206 -0.01387301 -0.00669345\n",
      "   -0.08556506]\n",
      "  [-0.03428832  0.02473707  0.00638186  0.00301592  0.03876719\n",
      "    0.02305264]\n",
      "  [-0.02576433 -0.00288253  0.05967318  0.04707334 -0.00611119\n",
      "    0.05196183]\n",
      "  [ 0.0235363  -0.04792032  0.03497241 -0.00968364 -0.04204199\n",
      "    0.00966486]]\n",
      "\n",
      " [[-0.00708105 -0.03494656 -0.06759453  0.05501465  0.04467877\n",
      "   -0.02714159]\n",
      "  [-0.0053491   0.0260518  -0.01416695  0.04234953  0.05477338\n",
      "   -0.02255105]\n",
      "  [-0.05188236 -0.03578881  0.00754474  0.01513701 -0.05179217\n",
      "    0.08367383]\n",
      "  [-0.02195654  0.00105239  0.06624406  0.01884438 -0.00712731\n",
      "    0.02803523]\n",
      "  [ 0.0235363  -0.04792032  0.03497241 -0.00968364 -0.04204199\n",
      "    0.00966486]]], shape=(2, 5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "my_embedding_layer = PositionEmbeddingLayer(output_sequence_length,\n",
    "                                           vocab_size, output_length)\n",
    "embedded_layer_output = my_embedding_layer(vectorized_words)\n",
    "print(\"Output from my_embedded_layer:\",embedded_layer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a3d17",
   "metadata": {},
   "source": [
    "### Positional Encoding in Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "da95ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "    def __init__(self, seq_length, vocab_size, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)\n",
    "        pos_embedding_matrix = self.get_position_encoding(seq_length, output_dim)\n",
    "\n",
    "        self.word_embedding_layer = Embedding(\n",
    "                                        input_dim = vocab_size,\n",
    "                                        output_dim = output_dim,\n",
    "                                        weights = [word_embedding_matrix],\n",
    "                                        trainable=False)\n",
    "        self.position_embedding_layer = Embedding(\n",
    "                                        input_dim = seq_length,\n",
    "                                        output_dim = output_dim,\n",
    "                                        weights = [pos_embedding_matrix],\n",
    "                                        trainable=False)\n",
    "    \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in range(int(d/2)):\n",
    "                denominator = np.power(n,2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9835d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from my_embedded_layer:  tf.Tensor(\n",
      "[[[-0.9589243   1.2836622   0.23000172  1.9731903   0.01077196\n",
      "    1.9999421 ]\n",
      "  [ 0.56205547  1.5004725   0.3213085   1.9603932   0.01508068\n",
      "    1.9999142 ]\n",
      "  [ 1.566284    0.3377554   0.41192317  1.9433732   0.01938933\n",
      "    1.999877  ]\n",
      "  [ 1.0504174  -1.4061394   0.2314966   1.9860148   0.01077211\n",
      "    1.9999698 ]\n",
      "  [-0.7568025   0.3463564   0.18459873  1.982814    0.00861763\n",
      "    1.9999628 ]]\n",
      "\n",
      " [[ 0.14112     0.0100075   0.1387981   1.9903207   0.00646326\n",
      "    1.9999791 ]\n",
      "  [ 0.08466846 -0.11334133  0.23099795  1.9817369   0.01077207\n",
      "    1.9999605 ]\n",
      "  [ 1.8185948  -0.8322937   0.185397    1.9913884   0.00861771\n",
      "    1.9999814 ]\n",
      "  [ 0.14112     0.0100075   0.1387981   1.9903207   0.00646326\n",
      "    1.9999791 ]\n",
      "  [-0.7568025   0.3463564   0.18459873  1.982814    0.00861763\n",
      "    1.9999628 ]]], shape=(2, 5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "attnisallyouneed_embedding = PositionEmbeddingFixedWeights(output_sequence_length,\n",
    "                                         vocab_size, output_length)\n",
    "attnisallyouneed_output = attnisallyouneed_embedding(vectorized_words)\n",
    "print(\"Output from my_embedded_layer: \", attnisallyouneed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95900948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be3635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd025a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
