{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed06771f",
   "metadata": {},
   "source": [
    "# Training Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb093abf",
   "metadata": {},
   "source": [
    "Dataset - https://github.com/Rishav09/Neural-Machine-Translation-System/blob/master/english-german-both.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23699e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy.random import shuffle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import convert_to_tensor, int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccd58b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/a.daggula/Downloads/english-german-both.pkl'\n",
    "clean_dataset = load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd38aeb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['i like both', 'ich mag beide'],\n",
       "       ['she misses him', 'er fehlt ihr'],\n",
       "       ['i followed him', 'ich folgte ihm'],\n",
       "       ['its unusual', 'es ist ungewohnlich'],\n",
       "       ['she sounded mad', 'sie klang wutend']], dtype='<U370')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_dataset[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1edbec51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fab34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset:\n",
    "    def __init__(self,**kwargs):\n",
    "        self.n_sentences = 9999\n",
    "        self.train_split = 0.9\n",
    "    \n",
    "    def create_tokenizer(self, dataset):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "        return tokenizer\n",
    "    \n",
    "    def find_seq_length(self, dataset):\n",
    "        return max(len(seq.split()) for seq in dataset)\n",
    "    \n",
    "    def find_vocab_size(self, tokenizer, dataset):\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "        return len(tokenizer.word_index) + 1\n",
    "    \n",
    "    def __call__(self, filename, **kwargs):\n",
    "        # Load a Dataset\n",
    "        clean_dataset = load(open(filename, 'rb'))\n",
    "        \n",
    "        # Reduce Dataset Size\n",
    "        dataset = clean_dataset[:self.n_sentences, :]\n",
    "        \n",
    "        # Add Start and Stop Tokens\n",
    "        for i in range(dataset[:,0].size):\n",
    "            dataset[i, 0] = \"<START>\" + dataset[i, 0] + \"<EOS>\"\n",
    "            dataset[i, 1] = \"<START>\" + dataset[i, 1] + \"<EOS>\"\n",
    "        \n",
    "        # Random Shuffle the dataset\n",
    "        shuffle(dataset)\n",
    "        \n",
    "        # Split the dataset\n",
    "        train = dataset[:int(self.n_sentences * self.train_split)]\n",
    "        \n",
    "        # Prepare tokenizer for the encoder input\n",
    "        enc_tokenizer = self.create_tokenizer(train[:, 0])\n",
    "        enc_seq_length = self.find_seq_length(train[:, 0])\n",
    "        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n",
    "        \n",
    "        # Encode and pad the input sequences\n",
    "        trainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n",
    "        trainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\n",
    "        trainX = convert_to_tensor(trainX, dtype=int64)\n",
    "        \n",
    "        #Prepare tokenizer for the decoder input\n",
    "        dec_tokenizer = self.create_tokenizer(train[:, 1])\n",
    "        dec_seq_length = self.find_seq_length(train[:, 1])\n",
    "        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n",
    "        \n",
    "        # Encode and pad the input sequences\n",
    "        trainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n",
    "        trainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\n",
    "        trainY = convert_to_tensor(trainY, dtype=int64)\n",
    "        \n",
    "        return (trainX, trainY, train, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d40c5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START>who bought this<EOS> \n",
      " tf.Tensor([  1  47 455  21   2], shape=(5,), dtype=int64)\n",
      "<START>tom war ein spion<EOS> \n",
      " tf.Tensor([   1    5   26   16 1005    2    0    0    0], shape=(9,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = PrepareDataset()\n",
    "trainX, trainY, train_orig, enc_seq_length, dec_seq_length, \\\n",
    "     enc_vocab_size, dec_vocab_size = dataset(filename)\n",
    "\n",
    "print(train_orig[20, 0], '\\n', trainX[20, :])\n",
    "print(train_orig[0, 1], '\\n', trainY[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7c26b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder sequence length: 5\n",
      "Decoder sequence length: 10\n"
     ]
    }
   ],
   "source": [
    "print('Encoder sequence length:', enc_seq_length)\n",
    "print('Decoder sequence length:', dec_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697bf26",
   "metadata": {},
   "source": [
    "Apply Padding Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c0d5ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fcn(target, prediction):\n",
    "    mask = math.logical_not(target, 0)\n",
    "    mask = cast(mask, float32)\n",
    "    \n",
    "    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * mask\n",
    "    return reduce_sum(loss) / reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae20399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fcn(target, prediction):\n",
    "    mask = math.logical_not(math.equal(target,0))\n",
    "    \n",
    "    # Find equal prediction and target values, and apply the padding mask\n",
    "    accuracy = equal(target, argmax(prediction, axis=2))\n",
    "    accuracy = math.logical_and(mask, accuracy)\n",
    "    # Cast the True/False values to 32-bit-precision floating-point numbers\n",
    "    mask = cast(mask, float32)\n",
    "    accuracy = cast(accuracy, float32)\n",
    "    # Compute the mean accuracy over the unmasked values\n",
    "    return reduce_sum(accuracy) / reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd3417",
   "metadata": {},
   "source": [
    "#### Train the transformer Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7dbc7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the model parameters\n",
    "h = 8 # Number of self-attention heads\n",
    "d_k = 64 # Dimensionality of the linearly projected queries and keys d_v = 64 # Dimensionality of the linearly projected values\n",
    "d_model = 512 # Dimensionality of model layers' outputs\n",
    "d_ff = 2048 # Dimensionality of the inner fully connected layer\n",
    "n = 6 # Number of layers in the encoder stack\n",
    "# Define the training parameters\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "dropout_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85cffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler(LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = cast(d_model, float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step_num):\n",
    "        # Linearly increasing the learning rate for the first warmup_steps, and # decreasing it thereafter\n",
    "        arg1 = step_num ** -0.5\n",
    "        arg2 = step_num * (self.warmup_steps ** -1.5)\n",
    "        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
