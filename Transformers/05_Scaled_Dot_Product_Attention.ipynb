{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455ac3d2",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86e20b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import matmul, math, cast, float32 \n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras.backend import softmax\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12f0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(Layer):\n",
    "    def __int__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self,queries,keys,values,d_k,mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling \n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None: scores += -1e9 * mask\n",
    "                 # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82abc0",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c247071",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = 64 # Dimensionality of the linearly projected queries and keys \n",
    "d_v = 64 # Dimensionality of the linearly projected values \n",
    "batch_size = 64 # Batch size from the training process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1903a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_length = 5 # Maximum length of the input sequence\n",
    "queries = random.random((batch_size, input_seq_length, d_k))\n",
    "keys = random.random((batch_size, input_seq_length, d_k))\n",
    "values = random.random((batch_size, input_seq_length, d_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2370b39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.17039935 0.5660157  0.49454716 ... 0.63606805 0.731504   0.46568075]\n",
      "  [0.17068729 0.5583192  0.5042453  ... 0.63850045 0.74013174 0.4734642 ]\n",
      "  [0.17546858 0.5731396  0.49296683 ... 0.6524726  0.72995955 0.47620583]\n",
      "  [0.17138234 0.5680681  0.490462   ... 0.6407421  0.7280969  0.47297087]\n",
      "  [0.17335843 0.5582293  0.49005768 ... 0.64072454 0.7280288  0.4742112 ]]\n",
      "\n",
      " [[0.39731753 0.40955257 0.4291552  ... 0.37298998 0.5416086  0.7824889 ]\n",
      "  [0.3965895  0.4205944  0.42082188 ... 0.37335843 0.5425651  0.79109925]\n",
      "  [0.35964045 0.40663934 0.42679647 ... 0.38441384 0.5283919  0.7844816 ]\n",
      "  [0.3748286  0.41582245 0.41625062 ... 0.3807641  0.5327348  0.7933047 ]\n",
      "  [0.35876435 0.39259434 0.39888084 ... 0.3928527  0.5334414  0.8034639 ]]\n",
      "\n",
      " [[0.16549972 0.28848526 0.4118323  ... 0.5087974  0.6443383  0.62525296]\n",
      "  [0.17033522 0.26947588 0.41163826 ... 0.511412   0.6396189  0.6172437 ]\n",
      "  [0.16947632 0.29201823 0.4199351  ... 0.49825054 0.64014316 0.615296  ]\n",
      "  [0.1696158  0.26359063 0.4089007  ... 0.5139334  0.63423836 0.6108542 ]\n",
      "  [0.16231154 0.2908795  0.41003826 ... 0.5082259  0.6187894  0.5989208 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.49852756 0.4291692  0.44770697 ... 0.4029108  0.24936879 0.7725136 ]\n",
      "  [0.50283736 0.4206116  0.43212843 ... 0.40470988 0.2474757  0.7658229 ]\n",
      "  [0.48658028 0.42418814 0.44522792 ... 0.40997446 0.25186324 0.76668715]\n",
      "  [0.503792   0.44374636 0.45816356 ... 0.39895207 0.25800088 0.77976155]\n",
      "  [0.50555766 0.42985094 0.4467361  ... 0.40674245 0.25017408 0.77206254]]\n",
      "\n",
      " [[0.61947083 0.3587406  0.4888078  ... 0.4793515  0.32555404 0.45666355]\n",
      "  [0.6074656  0.34856093 0.49532592 ... 0.4704468  0.3299029  0.45016137]\n",
      "  [0.6218989  0.36742434 0.48462087 ... 0.48743707 0.3249203  0.46293935]\n",
      "  [0.59500486 0.36811793 0.49067184 ... 0.48546037 0.33029008 0.46030805]\n",
      "  [0.61999065 0.35875267 0.48962045 ... 0.47732916 0.32317466 0.4540324 ]]\n",
      "\n",
      " [[0.4734035  0.28373665 0.53470117 ... 0.4579231  0.5219711  0.5248433 ]\n",
      "  [0.5047572  0.30464703 0.5311011  ... 0.4796396  0.537002   0.5347398 ]\n",
      "  [0.4935282  0.28406888 0.5212006  ... 0.44762465 0.5310913  0.54743516]\n",
      "  [0.48591894 0.2784659  0.523007   ... 0.45029873 0.5215613  0.5416466 ]\n",
      "  [0.49188334 0.28070843 0.5220851  ... 0.4617186  0.5167763  0.54109347]]], shape=(64, 5, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "attention = DotProductAttention()\n",
    "print(attention(queries, keys, values, d_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c1200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
